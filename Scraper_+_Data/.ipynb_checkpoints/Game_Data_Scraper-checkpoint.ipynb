{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbc54d2f-b151-4065-ad51-3af6a1a4b2b2",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c424417-e03e-47e2-921a-718fd33fabac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef14658-b19f-4e96-8d5f-e744349be73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "site = 'https://game8.co/games/Last-of-Us-2/archives/290290' # walkthrough page\n",
    "response = requests.get(site)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525119b3-fd6b-4cbd-8df7-7ff84923eee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939d804b-2ec8-4169-9a0d-58c5fa78594e",
   "metadata": {},
   "source": [
    "## Chapter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f215c1-cd7d-49ee-b0c1-3786324de5bf",
   "metadata": {},
   "source": [
    "This function is used to get all of the images links based on the alt text.\n",
    "\n",
    "So by making a loop to go through either data-src or src images links we get a list of images links and depending if the src begins with //, we can add the https or the full website url to make it a full url.\n",
    "\n",
    "Then it compares if any of the target names are found in the url alt texts, if yes then they are agged to the images link.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a765bec-0eb9-40a9-8bc7-612d7348fd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filtered_image_links(soup, target_names):\n",
    "    images = []\n",
    "    img_tags = soup.find_all('img')\n",
    "    \n",
    "    for img in img_tags:\n",
    "        img_url = img.get('data-src') or img.get('src')\n",
    "        \n",
    "        if img_url:\n",
    "            if img_url.startswith('//'):\n",
    "                img_url = 'https:' + img_url\n",
    "            elif img_url.startswith('/'):\n",
    "                img_url = 'https://game8.co' + img_url\n",
    "\n",
    "            alt_text = img.get('alt', '').lower()\n",
    "            \n",
    "            if any(target_name.lower() in alt_text for target_name in target_names):\n",
    "                images.append({\n",
    "                    'url': img_url,\n",
    "                    'alt': alt_text\n",
    "                })\n",
    "    \n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f0bf13-e5ca-4074-a30f-5d480aacc082",
   "metadata": {},
   "source": [
    "Make a list of names I need to specifically find and run the function.\n",
    "\n",
    "Then I run a loop through each image and compare their alt text to my find names text and make a dictionary key pair to store in the chapter_images dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4824a293-4ae9-4fff-a2db-dfc995a33169",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_names = [\n",
    "    'Jackson', 'Seattle Day 1', 'Seattle Day 2', 'Seattle Day 3', 'The Park', \n",
    "    'The Farm', 'Santa Barbara'\n",
    "]\n",
    "\n",
    "filtered_images = get_filtered_image_links(soup, find_names)\n",
    "\n",
    "chapter_images = {}\n",
    "for image in filtered_images:\n",
    "    for chapter in find_names:\n",
    "        if chapter.lower() in image['alt']:\n",
    "            chapter_images[chapter] = image['url']\n",
    "\n",
    "print(chapter_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcda596-6105-44a2-9312-13dd0e7c2a7f",
   "metadata": {},
   "source": [
    "This code is extracting both chapter names and their sub-chapters with urls.\n",
    "\n",
    "It finds teh chapter names -> processes each chapter by finding their sub-chapter and links them together\n",
    "\n",
    "At the end for each chapter it has a list of sub-chapter, each containtg a name and url, and storing them in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c661d93-c35f-4b7a-a17f-07597f65c56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chapters = soup.find_all('a', class_='list_contents')\n",
    "chapter_names = [chapter.get_text(strip=True) for chapter in chapters]\n",
    "\n",
    "base_url = \"https://game8.co\"\n",
    "sub_chapters_dict = {}\n",
    "\n",
    "chapters_header = soup.find_all('h3', class_='a-header--3')\n",
    "\n",
    "for chapter in chapters_header:\n",
    "    chapter_title = chapter.get_text(strip=True)\n",
    "    table = chapter.find_next('table', class_='a-table')\n",
    "    sub_chapters = table.find_all('td', class_='center')\n",
    "    \n",
    "    sub_chapters_list = []\n",
    "    \n",
    "    for sub_chapter in sub_chapters:\n",
    "        link = sub_chapter.find('a', class_='a-link')\n",
    "        if link:\n",
    "            sub_chapters_list.append({\n",
    "                'text': link.get_text(strip=True),\n",
    "                'url': base_url + link['href']\n",
    "            })\n",
    "    \n",
    "    sub_chapters_dict[chapter_title] = sub_chapters_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c258e5-b183-4f69-9b14-d81ecfe8420b",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data = []\n",
    "\n",
    "for chapter_key in sub_chapters_dict.keys():\n",
    "    chapter_name = chapter_key.split(':')[-1].strip()\n",
    "\n",
    "    row = {\n",
    "        'Chapter Name': chapter_key,  \n",
    "        'Chapter Image': chapter_images.get(chapter_name, 'No Image Available'), # placeholder if no image is found \n",
    "        'Sub-chapters': ', '.join([sub_chap['text'] for sub_chap in sub_chapters_dict.get(chapter_key, [])]),\n",
    "        'Sub-chapter URLs': ', '.join([sub_chap['url'] for sub_chap in sub_chapters_dict.get(chapter_key, [])]),\n",
    "    }\n",
    "    csv_data.append(row)\n",
    "\n",
    "\n",
    "csv_file = 'chapter_data.csv'\n",
    "\n",
    "with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=['Chapter Name', 'Chapter Image', 'Sub-chapters', 'Sub-chapter URLs'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(csv_data)\n",
    "\n",
    "print(f\"CSV file '{csv_file}' created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38922740-6036-439e-b71f-d1cf6db19c56",
   "metadata": {},
   "source": [
    "## Chapter walkthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd85050-0da6-4619-8656-79e661987792",
   "metadata": {},
   "outputs": [],
   "source": [
    "chapters_df = pd.read_csv('chapter_data.csv')\n",
    "chapters_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197180ec-fd6c-4cc8-9804-e94508821f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_walkthrough_data = []\n",
    "\n",
    "for index, row in chapters_df.iterrows():\n",
    "    chapter_name = row['Chapter Name']\n",
    "    sub_chapter_urls_string = row['Sub-chapter URLs']\n",
    "    sub_chapter_urls_list = sub_chapter_urls_string.split(', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed96e7e5-65cd-4b3a-8b3d-6cea16f772e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_walkthrough_data = []\n",
    "\n",
    "for index, row in chapters_df.iterrows():\n",
    "    chapter_name = row['Chapter Name']\n",
    "    sub_chapter_urls_string = row['Sub-chapter URLs']\n",
    "\n",
    "    sub_chapter_urls_list = sub_chapter_urls_string.split(', ')\n",
    "    sub_chapter_names_string = row['Sub-chapters']\n",
    "    sub_chapter_names_list = sub_chapter_names_string.split(', ')\n",
    "\n",
    "\n",
    "    main_chapter_name = chapter_name.split(':')[-1].strip()\n",
    "\n",
    "    for i, sub_chapter_url in enumerate(sub_chapter_urls_list):\n",
    "        try:\n",
    "            response = requests.get(sub_chapter_url)\n",
    "            response.raise_for_status()  \n",
    "\n",
    "            chapter_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            sub_chapter_tag = chapter_soup.find('h2', class_='a-header--2')\n",
    "            if sub_chapter_tag:\n",
    "                extracted_sub_chapter_name = sub_chapter_tag.get_text(strip=True)\n",
    "                extracted_sub_chapter_name = extracted_sub_chapter_name.replace(f\"{main_chapter_name} - \", \"\").strip()\n",
    "                extracted_sub_chapter_name = extracted_sub_chapter_name.replace(f\"{main_chapter_name}: \", \"\").strip()\n",
    "\n",
    "                sub_chapter_name = f\"{main_chapter_name} - {extracted_sub_chapter_name}\"\n",
    "            elif i < len(sub_chapter_names_list):\n",
    "                sub_chapter_name = f\"{main_chapter_name} - {sub_chapter_names_list[i].strip()}\"\n",
    "            else:\n",
    "                sub_chapter_name = 'N/A'\n",
    "\n",
    "\n",
    "            spans = chapter_soup.find_all('span', style=\"font-size:120%;\")\n",
    "            extracted_text = [span.get_text(strip=True) for span in spans]\n",
    "\n",
    "            formatted_text = []\n",
    "            if extracted_text:\n",
    "                first_item = extracted_text[0].rstrip('.')\n",
    "                formatted_text.append(first_item[0].upper() + first_item[1:].lower())\n",
    "\n",
    "                for item in extracted_text[1:]:\n",
    "                    formatted_text.append(item.lower())\n",
    "\n",
    "\n",
    "            grouped_text = []\n",
    "            for j in range(0, len(formatted_text), 2):\n",
    "                line_items = formatted_text[j:j+2]\n",
    "                if len(line_items) > 1:\n",
    "                    grouped_text.append(\", and \".join(line_items))\n",
    "                elif line_items:\n",
    "                    grouped_text.append(line_items[0])\n",
    "\n",
    "            all_walkthrough_data.append({\n",
    "                'Chapter Name': chapter_name,\n",
    "                'Sub-chapter Name': sub_chapter_name,\n",
    "                'Sub-chapter URL': sub_chapter_url,\n",
    "                'Walkthrough Text': \"\\n\".join(grouped_text)\n",
    "            })\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching {sub_chapter_url}: {e}\")\n",
    "            all_walkthrough_data.append({\n",
    "                'Chapter Name': chapter_name,\n",
    "                'Sub-chapter Name': f\"{main_chapter_name} - {sub_chapter_names_list[i].strip()}\" if i < len(sub_chapter_names_list) else 'N/A',\n",
    "                'Sub-chapter URL': sub_chapter_url,\n",
    "                'Walkthrough Text': f\"Error fetching data: {e}\"\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred while processing {sub_chapter_url}: {e}\")\n",
    "            all_walkthrough_data.append({\n",
    "                'Chapter Name': chapter_name,\n",
    "                'Sub-chapter Name': f\"{main_chapter_name} - {sub_chapter_names_list[i].strip()}\" if i < len(sub_chapter_names_list) else 'N/A',\n",
    "                'Sub-chapter URL': sub_chapter_url,\n",
    "                'Walkthrough Text': f\"An unexpected error occurred: {e}\"\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34708ec3-ce74-43a6-add5-7d0f873d26e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "walkthrough_df = pd.DataFrame(all_walkthrough_data)\n",
    "walkthrough_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2949cfd-f28f-4f2e-8dda-ef9e9531e6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "walkthrough_df.to_csv('walkthrough_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fec3cf-ef7e-4edc-90b2-e34ac532b7bb",
   "metadata": {},
   "source": [
    "## Tips and Tricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9fcde9-c15e-4f65-bfb0-e7e353faba95",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips = 'https://game8.co/games/Last-of-Us-2/archives/290295' # tips page\n",
    "response = requests.get(tips)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54811b44-b5a5-48d0-9e69-b814e3ec2c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c860ce4e-d36f-4a2f-b483-4a72765f37fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = [\n",
    "    'Combat Tips - Banner.jpg', 'Horse Riding - Jumping.jpg', 'Crafting Training Manual.jpg', 'Opening Safe.jpg', 'Guitar Chords.jpg'\n",
    "]\n",
    "\n",
    "filtered_images = get_filtered_image_links(tips_soup, target_names)\n",
    "\n",
    "tips_images = {}\n",
    "for image in filtered_images:\n",
    "    for tip_name in target_names:\n",
    "        if tip_name.lower() in image['alt']:\n",
    "            tips_images[tip_name.replace('.jpg', '').strip()] = image['url']\n",
    "            break\n",
    "\n",
    "print(tips_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d323c648-5505-4e5d-831a-ba4f07b94d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://game8.co\"\n",
    "sub_tips_dict = {}\n",
    "\n",
    "tips_header = tips_soup.find_all('h3', class_='a-header--3')\n",
    "\n",
    "tips_elements = tips_soup.find_all('a', class_='list_contents')\n",
    "tips_names = [tip.get_text(strip=True) for tip in tips_elements]\n",
    "\n",
    "\n",
    "for tip in tips_header:\n",
    "  tip_title = tip.get_text(strip=True)\n",
    "  table = tip.find_next('table', class_='a-table')\n",
    "  sub_tips = table.find_all('td', class_='center')\n",
    "\n",
    "  sub_tips_list = []\n",
    "\n",
    "  for sub_tip in sub_tips:\n",
    "    link = sub_tip.find('a', class_='a-link')\n",
    "    if link:\n",
    "      sub_tips_list.append({\n",
    "        'text': link.get_text(strip=True),\n",
    "        'url': base_url + link['href']\n",
    "      })\n",
    "\n",
    "  sub_tips_dict[tip_title] = sub_tips_list\n",
    "\n",
    "\n",
    "\n",
    "print(sub_tips_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838b4367-4e04-46b5-8542-e7d6c2a7ce07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tip_image_mapping = {\n",
    "    'Last of Us 2 Gameplay Guides': 'Combat Tips - Banner',\n",
    "    'Last of Us 2 Controls': 'Horse Riding - Jumping', \n",
    "    'Last of Us 2 Materials and Parts': 'Crafting Training Manual', \n",
    "    'Last of Us 2 Exploration and Secrets Guides': 'Opening Safe', \n",
    "    'Last of Us 2 Miscellaneous Guides': 'Guitar Chords' \n",
    "}\n",
    "\n",
    "\n",
    "print(\"Tip image mapping created:\")\n",
    "print(tip_image_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17096475-7f0d-46d7-9cc0-d09d0a5e7376",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data = []\n",
    "\n",
    "for tip_key in sub_tips_dict.keys():\n",
    "\n",
    "    image_key_from_mapping = tip_image_mapping.get(tip_key)\n",
    "\n",
    "\n",
    "    matching_image_url = tips_images.get(image_key_from_mapping, 'No Image Available')\n",
    "\n",
    "    row = {\n",
    "        'Tip Name': tip_key,\n",
    "        'Tip Image': matching_image_url,\n",
    "        'Sub-tips': ', '.join([sub_tip['text'] for sub_tip in sub_tips_dict.get(tip_key, [])]),\n",
    "        'Sub-tip URLs': ', '.join([sub_tip['url'] for sub_tip in sub_tips_dict.get(tip_key, [])]),\n",
    "    }\n",
    "    csv_data.append(row)\n",
    "\n",
    "\n",
    "csv_file = 'tips_data.csv'\n",
    "\n",
    "with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=['Tip Name', 'Tip Image', 'Sub-tips', 'Sub-tip URLs'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(csv_data)\n",
    "\n",
    "print(f\"CSV file '{csv_file}' created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9306b2b3-4fae-48a4-bf40-6b1f0a5711e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_df = pd.read_csv('tips_data.csv')\n",
    "tips_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d9c37a-6606-48d9-93e2-7482c6985e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tips_data = []\n",
    "\n",
    "for index, row in tips_df.iterrows():\n",
    "    tip_name = row['Tip Name']\n",
    "    sub_tip_urls_string = row['Sub-tip URLs']\n",
    "\n",
    "    sub_tips_urls_list = sub_tip_urls_string.split(', ')\n",
    "    sub_tip_name_string = row['Sub-tips']\n",
    "    sub_tip_name_list = sub_tip_name_string.split(', ')\n",
    "\n",
    "    main_tip_name = tip_name.split(':')[-1].strip()\n",
    "\n",
    "    for i, sub_tip_url in enumerate(sub_tips_urls_list):\n",
    "        try:\n",
    "            response = requests.get(sub_tip_url)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            sub_tip_soup = BeautifulSoup(response.text, 'html.parser') \n",
    "\n",
    "            \n",
    "            sub_chapter_tag = sub_tip_soup.find('h2', class_='a-header--2')\n",
    "            if sub_chapter_tag:\n",
    "                extracted_sub_tip_name = sub_chapter_tag.get_text(strip=True)\n",
    "                \n",
    "                extracted_sub_tip_name = extracted_sub_tip_name.replace(f\"{main_tip_name} - \", \"\").strip()\n",
    "                extracted_sub_tip_name = extracted_sub_tip_name.replace(f\"{main_tip_name}: \", \"\").strip()\n",
    "\n",
    "                sub_tip_name = f\"{main_tip_name} - {extracted_sub_tip_name}\"\n",
    "            elif i < len(sub_tip_name_list):\n",
    "                sub_tip_name = f\"{main_tip_name} - {sub_tip_name_list[i].strip()}\"\n",
    "            else:\n",
    "                sub_tip_name = 'N/A'\n",
    "\n",
    "            \n",
    "            detailed_tip_dict = {}\n",
    "            current_heading = None\n",
    "\n",
    "            \n",
    "            content_elements = sub_tip_soup.select('h2.a-header--2, h3.a-header--3, p.a-paragraph, table.a-table')\n",
    "\n",
    "            for element in content_elements:\n",
    "                if element.name in ['h2', 'h3']:\n",
    "                    current_heading = element.get_text(strip=True)\n",
    "                    detailed_tip_dict[current_heading] = [] \n",
    "                elif element.name == 'p' and 'a-paragraph' in element.get('class', []):\n",
    "                    if current_heading:\n",
    "                        detailed_tip_dict[current_heading].append(element.get_text(strip=True))\n",
    "                elif element.name == 'table' and 'a-table' in element.get('class', []):\n",
    "                     if current_heading:\n",
    "                        \n",
    "                        table_data = []\n",
    "                        header_row = element.find('tr')\n",
    "                        if header_row:\n",
    "                            headers = [th.get_text(strip=True) for th in header_row.find_all('th')]\n",
    "                            data_rows = element.find_all('tr')[1:] \n",
    "                            for row in data_rows:\n",
    "                                cells = [td.get_text(strip=True) for td in row.find_all('td')]\n",
    "                                if len(headers) == len(cells):\n",
    "                                    table_data.append(dict(zip(headers, cells)))\n",
    "                       \n",
    "                        detailed_tip_dict[current_heading].append({'table_data': table_data})\n",
    "\n",
    "\n",
    "           \n",
    "            formatted_detailed_tip_dict = {}\n",
    "            for heading, content_list in detailed_tip_dict.items():\n",
    "                paragraph_content = [item for item in content_list if not isinstance(item, dict) or 'table_data' not in item]\n",
    "                table_content = [item for item in content_list if isinstance(item, dict) and 'table_data' in item]\n",
    "\n",
    "                heading_data = {}\n",
    "                if paragraph_content:\n",
    "                    heading_data['paragraphs'] = \"\\n\".join(paragraph_content)\n",
    "                if table_content:\n",
    "                    heading_data['tables'] = table_content\n",
    "\n",
    "                formatted_detailed_tip_dict[heading] = heading_data\n",
    "\n",
    "\n",
    "            all_tips_data.append({\n",
    "                'Tip Name': tip_name,\n",
    "                'Sub-tip Name': sub_tip_name,\n",
    "                'Sub-tip URL': sub_tip_url,\n",
    "                'Detailed Tip Text': formatted_detailed_tip_dict \n",
    "            })\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching {sub_tip_url}: {e}\")\n",
    "            all_tips_data.append({\n",
    "                'Tip Name': tip_name,\n",
    "                'Sub-tip Name': f\"{main_tip_name} - {sub_tip_name_list[i].strip()}\" if i < len(sub_tip_name_list) else 'N/A',\n",
    "                'Sub-tip URL': sub_tip_url,\n",
    "                'Detailed Tip Text': f\"Error fetching data: {e}\"\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred while processing {sub_tip_url}: {e}\")\n",
    "            all_tips_data.append({\n",
    "                'Tip Name': tip_name,\n",
    "                'Sub-tip Name': f\"{main_tip_name} - {sub_tip_name_list[i].strip()}\" if i < len(sub_tip_name_list) else 'N/A',\n",
    "                'Sub-tip URL': sub_tip_url,\n",
    "                'Detailed Tip Text': f\"An unexpected error occurred: {e}\"\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156a8dc8-80e0-493d-b538-d6c0ad1d70be",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_df = pd.DataFrame(all_tips_data)\n",
    "tips_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f125f6c-211a-4f07-83cd-cc2858995df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_df.to_csv('detailed_tips_data.csv', index=False)\n",
    "print(\"Detailed tips data saved to 'detailed_tips_data.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc6b9cb-a036-43b8-8824-eaac37f4001d",
   "metadata": {},
   "source": [
    "## Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e37fd6b-27be-4ee2-86c0-aa968e243695",
   "metadata": {},
   "outputs": [],
   "source": [
    "character = 'https://game8.co/games/Last-of-Us-2/archives/290477' #character page\n",
    "response = requests.get(character)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519725df-7448-467b-bd16-b2db8b7d4a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f344a19d-7335-4d35-8a31-87f42dfec379",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = [\n",
    "    'Ellie Icon.png',\n",
    "    'Dina Icon.png',\n",
    "    'Joel Icon.png',\n",
    "    'Jesse Icon.png',\n",
    "    'Tommy Icon.png',\n",
    "    'Maria Icon.png',\n",
    "    'Seth Icon.png',\n",
    "    'Abby Icon.png',\n",
    "    'Owen Icon.png',\n",
    "    'Mel Icon.png',\n",
    "    'Nora Icon.png',\n",
    "    'Manny Icon.png',\n",
    "    'Jordan Icon.png',\n",
    "    'Isaac Icon.png',\n",
    "    'Alice Icon.png',\n",
    "    'Jerry Icon.png',\n",
    "    'Whitney Icon.png',\n",
    "    'Nick Icon.png',\n",
    "    'Lev Icon.png',\n",
    "    'Yara Icon.png',\n",
    "    'Emily Icon.png'\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "filtered_images = get_filtered_image_links(character_soup, target_names)\n",
    "\n",
    "character_images = {}\n",
    "for image in filtered_images:\n",
    "    for character_name_with_icon in target_names:\n",
    "        if character_name_with_icon.lower() in image['alt']:\n",
    "            # Extract just the character name by removing ' Icon.png'\n",
    "            character_name = character_name_with_icon.replace(' Icon.png', '').strip()\n",
    "            character_images[character_name] = image['url']\n",
    "            break\n",
    "\n",
    "\n",
    "print(character_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01b52f1-6928-4da1-8991-671a7a2a354b",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_main_image = {'Ellie' : 'https://img.game8.co/3253674/d94845ae35742fb7aeab666c545ce17c.jpeg/show',\n",
    "                          'Joel' : 'https://img.game8.co/3253664/103b20839a9d395c4391a3465867cdcb.jpeg/show',\n",
    "                          'Dina' : 'https://img.game8.co/3253702/457134b322ef240fbf357b4f3e2022bb.jpeg/show',\n",
    "                          'Abby' : 'https://img.game8.co/3253680/e37a8d8fc6ff287dbf28926e004aebd3.jpeg/show',\n",
    "                          'Yara' : 'https://img.game8.co/3253753/a53e019404ad3d253ca442e87e2b5270.jpeg/show',\n",
    "                          'Nora' : 'https://img.game8.co/3253701/527bfd2d36d8b19d5dd586499b79b98f.jpeg/show',\n",
    "                          'Lev_' : 'https://img.game8.co/3253754/95629e336e9e7bc1a7f9ffd871a12089.jpeg/show',\n",
    "                          'Emily' : 'https://img.game8.co/3253750/b64de7fa1386b449733d20ec2343fc69.jpeg/show',\n",
    "                          'Jesse' : 'https://img.game8.co/3253666/cd8a49d1e146c09662f466f1417b8063.jpeg/show',\n",
    "                          'Tommy' : 'https://img.game8.co/3253667/5f6bdd9e98558a4b3a8d18fd39c8d45a.jpeg/show',\n",
    "                          'Seth' : 'https://img.game8.co/3253700/442701af05e3748b9ccedc8a793a0000.jpeg/show',\n",
    "                          'Mel' : 'https://img.game8.co/3253673/43e4ae0186bd6083dc0b0f1bda3d6fae.jpeg/show',\n",
    "                          'Owen' : 'https://img.game8.co/3253668/37cb30d83159273a09bcd4c3eac43dc1.jpeg/show',\n",
    "                          'Manny' : 'https://img.game8.co/3253714/59d1b2cada49306c655dc50b5690e78e.jpeg/show',\n",
    "                          'Jordan' : 'https://img.game8.co/3253748/db9e05b070614c194c9d8e910f158883.jpeg/show',\n",
    "                          'Nick' : 'https://img.game8.co/3253751/0cbbe465209bd878cc3c4fe7b134b961.jpeg/show',\n",
    "                          'Alice' : 'https://img.game8.co/3253752/40349185b76d1d125192c59788f9e7a9.jpeg/show',\n",
    "                          'Maria' : 'https://img.game8.co/3253871/34d5cb007beec80ed621a67594bcd565.jpeg/show',\n",
    "                          'Jerry' : 'https://img.game8.co/3253888/433549ea350fab01b217e903199f165d.jpeg/show',\n",
    "                          'Whitney' : 'https://img.game8.co/3253876/211eb1e5683a5d9ae17872049440fc81.jpeg/show',\n",
    "                          'Isaac' : 'https://img.game8.co/3253872/66e409e5ea1a6c1649ac10146a013d70.jpeg/show'\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4976962-121c-4f24-a5c6-e2e581dc7553",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_profile_URLs = {'Ellie_page' : 'https://game8.co/games/Last-of-Us-2/archives/290451',\n",
    "                          'Joel_page' : 'https://game8.co/games/Last-of-Us-2/archives/290454',\n",
    "                          'Dina_page' : 'https://game8.co/games/Last-of-Us-2/archives/290477',\n",
    "                          'Abby_page' : 'https://game8.co/games/Last-of-Us-2/archives/290497',\n",
    "                          'Yara_page' : 'https://game8.co/games/Last-of-Us-2/archives/290493',\n",
    "                          'Nora_page' : 'https://game8.co/games/Last-of-Us-2/archives/290491',\n",
    "                          'Lev_page' : 'https://game8.co/games/Last-of-Us-2/archives/290495',\n",
    "                          'Emily_page' : 'https://game8.co/games/Last-of-Us-2/archives/290496',\n",
    "                          'Jesse_page' : 'https://game8.co/games/Last-of-Us-2/archives/290486',\n",
    "                          'Tommy_page' : 'https://game8.co/games/Last-of-Us-2/archives/290458',\n",
    "                          'Seth_page' : 'https://game8.co/games/Last-of-Us-2/archives/291199',\n",
    "                          'Mel_page' : 'https://game8.co/games/Last-of-Us-2/archives/291200',\n",
    "                          'Owen_page' : 'https://game8.co/games/Last-of-Us-2/archives/291205',\n",
    "                          'Manny_page' : 'https://game8.co/games/Last-of-Us-2/archives/291206',\n",
    "                          'Jordan_page' : 'https://game8.co/games/Last-of-Us-2/archives/291209',\n",
    "                          'Nick_page' : 'https://game8.co/games/Last-of-Us-2/archives/291210',\n",
    "                          'Alice_page' : 'https://game8.co/games/Last-of-Us-2/archives/291211',\n",
    "                          'Maria_page' : 'https://game8.co/games/Last-of-Us-2/archives/291219',\n",
    "                          'Jerry_page' : 'https://game8.co/games/Last-of-Us-2/archives/291220',\n",
    "                          'Whitney_page' : 'https://game8.co/games/Last-of-Us-2/archives/291221',\n",
    "                          'Isaac_page' : 'https://game8.co/games/Last-of-Us-2/archives/291222'\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816237a7-ff37-46af-9db8-783146492d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_details_list = []\n",
    "\n",
    "for character_name_with_suffix, profile_url in character_profile_URLs.items():\n",
    "    try:\n",
    "        response = requests.get(profile_url)\n",
    "        response.raise_for_status() # Raise an exception for bad status codes\n",
    "\n",
    "        profile_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract the character name without the '_page' suffix\n",
    "        character_name = character_name_with_suffix.replace('_page', '').strip()\n",
    "\n",
    "\n",
    "        # --- Extract Character Information based on the provided structure ---\n",
    "        character_info_dict = {}\n",
    "        current_heading = None\n",
    "\n",
    "        # Find all relevant heading, paragraph, and table tags in order\n",
    "        content_elements = profile_soup.select('h2.a-header--2, h3.a-header--3, p.a-paragraph, table.a-table')\n",
    "\n",
    "        for element in content_elements:\n",
    "            if element.name in ['h2', 'h3']:\n",
    "                current_heading = element.get_text(strip=True)\n",
    "                character_info_dict[current_heading] = [] # Initialize a list for content under this heading\n",
    "            elif element.name == 'p' and 'a-paragraph' in element.get('class', []):\n",
    "                # Append the paragraph text to the list under the current heading if a heading was found\n",
    "                if current_heading:\n",
    "                    character_info_dict[current_heading].append(element.get_text(strip=True))\n",
    "            elif element.name == 'table' and 'a-table' in element.get('class', []):\n",
    "                # Check for specific table classes if needed, but targeting 'a-table' seems sufficient based on description\n",
    "                 if current_heading:\n",
    "                    # Extract table data\n",
    "                    table_data = []\n",
    "                    header_row = element.find('tr')\n",
    "                    if header_row:\n",
    "                        headers = [th.get_text(strip=True) for th in header_row.find_all('th')]\n",
    "                        data_rows = element.find_all('tr')[1:] # Skip header row\n",
    "                        for row in data_rows:\n",
    "                            cells = [td.get_text(strip=True) for td in row.find_all('td')]\n",
    "                            if len(headers) == len(cells):\n",
    "                                table_data.append(dict(zip(headers, cells)))\n",
    "                    # Add table data to the list under the current heading, maybe with a key to indicate it's a table\n",
    "                    if table_data: # Only add if table data was actually extracted\n",
    "                         character_info_dict[current_heading].append({'table_data': table_data})\n",
    "\n",
    "\n",
    "        # --- Format the extracted information ---\n",
    "        # Join paragraphs under each heading and include tables\n",
    "        formatted_character_info = {}\n",
    "        for heading, content_list in character_info_dict.items():\n",
    "            paragraph_content = [item for item in content_list if not isinstance(item, dict) or 'table_data' not in item]\n",
    "            table_content = [item for item in content_list if isinstance(item, dict) and 'table_data' in item]\n",
    "\n",
    "            heading_data = {}\n",
    "            if paragraph_content:\n",
    "                heading_data['paragraphs'] = \"\\n\".join(paragraph_content)\n",
    "            if table_content:\n",
    "                heading_data['tables'] = table_content\n",
    "\n",
    "            if heading_data: # Only include the heading in the formatted output if it has content\n",
    "                formatted_character_info[heading] = heading_data\n",
    "\n",
    "\n",
    "        # --- Combine with Image URLs ---\n",
    "        # Get the character's icon image URL from the character_images dictionary using the cleaned character_name\n",
    "        icon_image_url = character_images.get(character_name, 'No Icon Image Available')\n",
    "        # Get the character's main image URL from the character_main_image dictionary\n",
    "        main_image_url = character_main_image.get(character_name, 'No Main Image Available')\n",
    "\n",
    "\n",
    "        # Append the collected data for this character\n",
    "        character_details_list.append({\n",
    "            'Character Name': character_name,\n",
    "            'Character Icon URL': icon_image_url,\n",
    "            'Character Main Image URL': main_image_url, # Added main image URL\n",
    "            'Profile URL': profile_url,\n",
    "            'Character Info': formatted_character_info # Store the structured information dictionary\n",
    "        })\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {profile_url}: {e}\")\n",
    "        # Use the cleaned character_name for the error entry as well\n",
    "        cleaned_name = character_name_with_suffix.replace('_page', '').strip()\n",
    "        character_details_list.append({\n",
    "            'Character Name': cleaned_name,\n",
    "            'Character Icon URL': character_images.get(cleaned_name, 'No Icon Image Available'),\n",
    "            'Character Main Image URL': character_main_image.get(cleaned_name, 'No Main Image Available'), # Added main image URL for error case\n",
    "            'Profile URL': profile_url,\n",
    "            'Character Info': f\"Error fetching data: {e}\", # Indicate error in info field\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while processing {profile_url}: {e}\")\n",
    "        # Use the cleaned character_name for the error entry as well\n",
    "        cleaned_name = character_name_with_suffix.replace('_page', '').strip()\n",
    "        character_details_list.append({\n",
    "            'Character Name': cleaned_name,\n",
    "            'Character Icon URL': character_images.get(cleaned_name, 'No Icon Image Available'),\n",
    "            'Character Main Image URL': character_main_image.get(cleaned_name, 'No Main Image Available'), # Added main image URL for error case\n",
    "            'Profile URL': profile_url,\n",
    "            'Character Info': f\"An unexpected error occurred: {e}\", # Indicate error in info field\n",
    "        })\n",
    "\n",
    "# Now character_details_list contains the scraped data for each character\n",
    "# The next step is to convert this into a DataFrame and save to CSV\n",
    "print(f\"Finished scraping details for {len(character_details_list)} characters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1be8751-0585-4419-9e4a-326f990d368c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of dictionaries into a pandas DataFrame\n",
    "character_df = pd.DataFrame(character_details_list)\n",
    "\n",
    "# Display the first few rows to verify the data\n",
    "display(character_df.head())\n",
    "\n",
    "# Define the CSV file name\n",
    "csv_file = 'character_data.csv'\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "character_df.to_csv(csv_file, index=False)\n",
    "\n",
    "print(f\"Character details saved to '{csv_file}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a217377b-53e7-4908-b75b-ac8532b4afc0",
   "metadata": {},
   "source": [
    "## Safe Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862ea08e-8167-42cf-b2fa-640b288899ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "safe_codes = 'https://game8.co/games/Last-of-Us-2/archives/290690' #safecode page\n",
    "response = requests.get(safe_codes)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea39b81-e903-4635-bc16-8b52ef6e48c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "safecode_soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb08e630-9250-4719-90ae-a83dd2ef95d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Provided safe code data\n",
    "safe_codes_data = {\n",
    "    'Chapter': [\n",
    "        'Jackson - Patrol', 'Seattle Day 1 - Downtown', 'Seattle Day 1 - Downtown',\n",
    "        'Seattle Day 1 - Downtown', 'Seattle Day 1 - Capitol Hill', 'Seattle Day 1 - Tunnels',\n",
    "        'Seattle Day 2 - Hillcrest', 'Seattle Day 2 - The Seraphites', 'Seattle Day 2 - The Seraphites',\n",
    "        'Seattle Day 3 - The Flooded City', 'Seattle Day 1 - On Foot', 'Seattle Day 1 - Hostile Territory',\n",
    "        'Seattle Day 1 - The Coast', 'Seattle Day 2 - The Shortcut', 'Seattle Day 2 - The Descent'\n",
    "    ],\n",
    "    'Location': [\n",
    "        'Super Market', 'Bank Vault', 'Courthouse', 'West Gate 2', 'Thrift Store',\n",
    "        'Locker Room', 'Auto Repair Shop', 'Apartment', 'Weston\\'s Pharmacy', 'First Gate',\n",
    "        'Big Win Safe', 'Jasmine Bakery', 'Boat Control Room', 'Apartment Bedroom', 'Across From Gym'\n",
    "    ],\n",
    "    'Combination/Code': [\n",
    "        '07-20-13', '60-23-06', '86-07-22', '04-51', '55-01-33', '15243', '30-82-65',\n",
    "        '10-08-83', '38-55-23', '70-12-64', '17-38-07', '68-96-89', '90-77-01',\n",
    "        '30-23-04', '12-18-79'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "safe_codes_df = pd.DataFrame(safe_codes_data)\n",
    "\n",
    "# Display the DataFrame\n",
    "display(safe_codes_df)\n",
    "\n",
    "# Save to CSV\n",
    "csv_file = 'safe_codes_data.csv'\n",
    "safe_codes_df.to_csv(csv_file, index=False)\n",
    "\n",
    "print(f\"CSV file '{csv_file}' created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844a3ab3-467e-4f44-abce-db277480bb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "safe_codes_locations = {\n",
    "  'Super Market' : 'https://game8.co/games/Last-of-Us-2/archives/290864',\n",
    "  'Bank Vault' : 'https://game8.co/games/Last-of-Us-2/archives/290681',\n",
    "  'Courthouse' : 'https://game8.co/games/Last-of-Us-2/archives/290860',\n",
    "  'West Gate 2': 'https://game8.co/games/Last-of-Us-2/archives/290844',\n",
    "  'Thrift Store': 'https://game8.co/games/Last-of-Us-2/archives/290825',\n",
    "  'Locker Room': 'https://game8.co/games/Last-of-Us-2/archives/290676',\n",
    "  'Auto Repair Shop': 'https://game8.co/games/Last-of-Us-2/archives/290837',\n",
    "  'Apartment': 'https://game8.co/games/Last-of-Us-2/archives/290854',\n",
    "  'Weston\\'s Pharmacy': 'https://game8.co/games/Last-of-Us-2/archives/290917',\n",
    "  'First Gate': 'https://game8.co/games/Last-of-Us-2/archives/290923',\n",
    "  'Big Win Safe': 'https://game8.co/games/Last-of-Us-2/archives/291118',\n",
    "  'Jasmine Bakery': 'https://game8.co/games/Last-of-Us-2/archives/291124',\n",
    "  'Boat Control Room': 'https://game8.co/games/Last-of-Us-2/archives/291129',\n",
    "  'Apartment Bedroom': 'https://game8.co/games/Last-of-Us-2/archives/291135',\n",
    "  'Across From Gym': 'https://game8.co/games/Last-of-Us-2/archives/291158'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4be3031-d47c-402d-b22a-37d637cc78bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "safe_locations_details = []\n",
    "\n",
    "\n",
    "for location, url in safe_codes_locations.items():\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status() # Raise an exception for bad status codes\n",
    "\n",
    "        location_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Dictionary to store details for this location\n",
    "        details = {\n",
    "            'Location': location,\n",
    "            'URL': url,\n",
    "            'Images': [],\n",
    "            'Paragraph Text': [],\n",
    "            'Headings': {},\n",
    "            'Steps to Safe': [] # To store steps under h3 tags\n",
    "        }\n",
    "\n",
    "        # Find the archive-style-wrapper div\n",
    "        archive_wrapper = location_soup.find('div', class_='archive-style-wrapper')\n",
    "\n",
    "        if archive_wrapper:\n",
    "            # Iterate through the contents of the archive_wrapper\n",
    "            for element in archive_wrapper.contents:\n",
    "                # Stop if we reach or pass an h3 with id 'hm_3'\n",
    "                if element.name == 'h3' and element.get('id') == 'hm_3':\n",
    "                    break\n",
    "\n",
    "                # Extract paragraph text\n",
    "                if element.name == 'p' and 'a-paragraph' in element.get('class', []):\n",
    "                     # Extract all text content from the paragraph, including nested tags\n",
    "                    paragraph_text = \"\".join(content.get_text(strip=True) for content in element.contents if isinstance(content, str) or content.name in ['span', 'b'])\n",
    "                    if paragraph_text:\n",
    "                        details['Paragraph Text'].append(paragraph_text)\n",
    "\n",
    "                    # Extract image URLs within this paragraph\n",
    "                    img_tags = element.find_all('img')\n",
    "                    for img in img_tags:\n",
    "                        img_url = img.get('data-src') or img.get('src')\n",
    "                        if img_url:\n",
    "                            if img_url.startswith('//'):\n",
    "                                img_url = 'https:' + img_url\n",
    "                            elif img_url.startswith('/'):\n",
    "                                img_url = 'https://game8.co' + img_url\n",
    "                            details['Images'].append(img_url)\n",
    "\n",
    "                # Extract images that might be direct children of the wrapper (less common but possible)\n",
    "                elif element.name == 'img':\n",
    "                     img_url = element.get('data-src') or element.get('src')\n",
    "                     if img_url:\n",
    "                         if img_url.startswith('//'):\n",
    "                             img_url = 'https:' + img_url\n",
    "                         elif img_url.startswith('/'):\n",
    "                             img_url = 'https://game8.co' + img_url\n",
    "                         details['Images'].append(img_url)\n",
    "\n",
    "\n",
    "            # Continue with extracting specific headings and steps to safe as before\n",
    "            # Extract H2 headings with specific IDs (these might be outside the wrapper or within it)\n",
    "            h2_hl1 = location_soup.find('h2', id='hl_1', class_='a-header--2')\n",
    "            if h2_hl1:\n",
    "                details['Headings']['hl_1'] = h2_hl1.get_text(strip=True)\n",
    "\n",
    "            h2_hl2 = location_soup.find('h2', id='hl_2', class_='a-header--2')\n",
    "            if h2_hl2:\n",
    "                details['Headings']['hl_2'] = h2_hl2.get_text(strip=True)\n",
    "\n",
    "            # Extract Steps to Safe under h3 tags (hm_1, hm_3, and subsequent)\n",
    "            # Find all h3 tags with class 'a-header--3'\n",
    "            h3_tags = location_soup.find_all('h3', class_='a-header--3')\n",
    "            current_step_heading = None\n",
    "            for h3 in h3_tags:\n",
    "                h3_text = h3.get_text(strip=True)\n",
    "                # Consider h3 with id 'hm_1' or 'hm_3' as starting points for steps\n",
    "                if h3.get('id') in ['hm_1', 'hm_3'] or current_step_heading:\n",
    "                     current_step_heading = h3_text\n",
    "                     step_content = {'heading': h3_text, 'paragraphs': [], 'images': []}\n",
    "\n",
    "                     # Find subsequent paragraphs and images until the next heading (h2 or h3) or end of content\n",
    "                     next_sibling = h3.next_sibling\n",
    "                     while next_sibling:\n",
    "                         if next_sibling.name in ['h2', 'h3']:\n",
    "                             break # Stop if we hit another heading\n",
    "                         if next_sibling.name == 'p' and 'a-paragraph' in next_sibling.get('class', []):\n",
    "                             step_content['paragraphs'].append(next_sibling.get_text(strip=True))\n",
    "                             # Find images within this paragraph as well\n",
    "                             step_img_tags = next_sibling.find_all('img')\n",
    "                             for img in step_img_tags:\n",
    "                                 img_url = img.get('data-src') or img.get('src')\n",
    "                                 if img_url:\n",
    "                                     if img_url.startswith('//'):\n",
    "                                         img_url = 'https:' + img_url\n",
    "                                     elif img_url.startswith('/'):\n",
    "                                         img_url = 'https://game8.co' + img_url\n",
    "                                     step_content['images'].append(img_url)\n",
    "\n",
    "                         next_sibling = next_sibling.next_sibling\n",
    "\n",
    "                     details['Steps to Safe'].append(step_content)\n",
    "\n",
    "\n",
    "        safe_locations_details.append(details)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        safe_locations_details.append({\n",
    "            'Location': location,\n",
    "            'URL': url,\n",
    "            'Error': f\"Error fetching data: {e}\"\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while processing {url}: {e}\")\n",
    "        safe_locations_details.append({\n",
    "            'Location': location,\n",
    "            'URL': url,\n",
    "            'Error': f\"An unexpected error occurred: {e}\"\n",
    "        })\n",
    "\n",
    "# Now safe_locations_details is a list of dictionaries containing the scraped details for each location\n",
    "# You can inspect this list or convert it to a DataFrame\n",
    "print(f\"Finished scraping details for {len(safe_locations_details)} safe locations.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ca0938-4601-4184-b064-ff43acf16069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the safe_locations_details list into a DataFrame\n",
    "safe_locations_df = pd.DataFrame(safe_locations_details)\n",
    "\n",
    "# Display the first few rows of the safe_locations_df to verify\n",
    "display(safe_locations_df.head())\n",
    "\n",
    "# Merge the safe_codes_df with the safe_locations_df based on the 'Location' column\n",
    "# Assuming 'Location' is the common column in both DataFrames\n",
    "merged_safe_data_df = pd.merge(safe_codes_df, safe_locations_df, on='Location', how='left')\n",
    "\n",
    "# Display the first few rows of the merged DataFrame to verify\n",
    "display(merged_safe_data_df.head())\n",
    "\n",
    "# Define the CSV file name for the combined data\n",
    "csv_file = 'safe_codes_.csv'\n",
    "\n",
    "# Save the merged DataFrame to a CSV file\n",
    "merged_safe_data_df.to_csv(csv_file, index=False)\n",
    "\n",
    "print(f\"Combined safe codes data saved to '{csv_file}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a56d978-1881-4bd5-9257-9da82033fdc0",
   "metadata": {},
   "source": [
    "## Trohpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78936a7b-1718-4c15-b344-6b613166841b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trophy = 'https://game8.co/games/Last-of-Us-2/archives/290658' #trophy page\n",
    "response = requests.get(trophy)\n",
    "response  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef09314d-db06-4df4-bb92-13b33f9d9aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "trophy_soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8065029a-8b7a-411a-a3c1-c972e8d0698c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trophy_details = {}\n",
    "\n",
    "# List of h3 IDs to target\n",
    "target_h3_ids = ['hm_1', 'hm_2', 'hm_3', 'hm_4', 'hm_5']\n",
    "\n",
    "for h3_id in target_h3_ids:\n",
    "    # Find the specific h3 tag by its ID and class\n",
    "    h3_tag = trophy_soup.find('h3', id=h3_id, class_='a-header--3')\n",
    "\n",
    "    if h3_tag:\n",
    "        heading_text = h3_tag.get_text(strip=True)\n",
    "\n",
    "        # Find the table immediately following this h3 tag\n",
    "        trophy_table = h3_tag.find_next_sibling('table', class_='a-table')\n",
    "\n",
    "        if trophy_table:\n",
    "            # Extract data from the table\n",
    "            table_data = []\n",
    "            # Assuming the table structure is consistent: image in first td, then th for title, then td for description\n",
    "            rows = trophy_table.find_all('tr')\n",
    "\n",
    "            current_trophy = {}\n",
    "            for row in rows:\n",
    "                # Extract image URL from the first td if present\n",
    "                img_tag = row.find('img')\n",
    "                if img_tag:\n",
    "                    img_url = img_tag.get('data-src') or img_tag.get('src')\n",
    "                    if img_url:\n",
    "                         if img_url.startswith('//'):\n",
    "                             img_url = 'https:' + img_url\n",
    "                         elif img_url.startswith('/'):\n",
    "                             img_url = 'https://game8.co' + img_url\n",
    "                         current_trophy['image'] = img_url\n",
    "\n",
    "                # Extract trophy title from th\n",
    "                th_tag = row.find('th')\n",
    "                if th_tag:\n",
    "                    current_trophy['title'] = th_tag.get_text(strip=True)\n",
    "\n",
    "                # Extract trophy description from td (excluding the td with the image)\n",
    "                td_tags = row.find_all('td')\n",
    "                # Find td that does NOT contain an img tag\n",
    "                description_td = None\n",
    "                for td in td_tags:\n",
    "                    if not td.find('img'):\n",
    "                        description_td = td\n",
    "                        break\n",
    "\n",
    "                if description_td:\n",
    "                    # Extract all text content from the description td and join with spaces\n",
    "                    description_text_parts = [content.strip() for content in description_td.contents if isinstance(content, str)]\n",
    "                    current_trophy['description'] = \" \".join(description_text_parts).strip()\n",
    "\n",
    "                    # If we have a title and description, this is likely a complete trophy entry\n",
    "                    if 'title' in current_trophy and 'description' in current_trophy:\n",
    "                         table_data.append(current_trophy)\n",
    "                         current_trophy = {} # Reset for the next trophy entry\n",
    "\n",
    "\n",
    "            # Store the extracted table data under the heading text\n",
    "            trophy_details[heading_text] = table_data\n",
    "        else:\n",
    "            print(f\"No table found after h3 with id='{h3_id}'\")\n",
    "    else:\n",
    "        print(f\"h3 with id='{h3_id}' not found\")\n",
    "\n",
    "# Display the extracted trophy details\n",
    "display(trophy_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd42a2c-9067-4eb8-b91e-9780ab886f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Prepare data for CSV\n",
    "csv_data = []\n",
    "for category, trophies in trophy_details.items():\n",
    "    for trophy in trophies:\n",
    "        csv_data.append({\n",
    "            'Trophy Category': category,\n",
    "            'Image URL': trophy.get('image', ''),\n",
    "            'Title': trophy.get('title', ''),\n",
    "            'Description': trophy.get('description', '')\n",
    "        })\n",
    "\n",
    "# Define CSV file name\n",
    "csv_file = 'trophy_data.csv'\n",
    "\n",
    "# Write data to CSV\n",
    "with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=['Trophy Category', 'Image URL', 'Title', 'Description'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(csv_data)\n",
    "\n",
    "print(f\"Trophy data saved to '{csv_file}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c9d0ef-e533-4dcf-8451-4b70b61d9430",
   "metadata": {},
   "source": [
    "## Weapons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215a4046-4d74-4b3f-95ac-601825786b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "weapons = 'https://game8.co/games/Last-of-Us-2/archives/290291' #weapons page\n",
    "response = requests.get(weapons)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fa4545-b06a-401b-b23f-71a8071689d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "weapons_soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19199065-525b-4fde-8356-d195a103e83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "weapons_data = {}\n",
    "\n",
    "# List of h3 IDs to target for weapon categories\n",
    "target_h3_ids = ['hm_1', 'hm_2', 'hm_3', 'hm_4', 'hm_5']\n",
    "\n",
    "for h3_id in target_h3_ids:\n",
    "    # Find the specific h3 tag by its ID and class\n",
    "    h3_tag = weapons_soup.find('h3', id=h3_id, class_='a-header--3')\n",
    "\n",
    "    if h3_tag:\n",
    "        heading_text = h3_tag.get_text(strip=True)\n",
    "\n",
    "        # Find the table immediately following this h3 tag\n",
    "        weapon_table = h3_tag.find_next_sibling('table', class_='a-table') # Use class 'a-table' as shown in structure\n",
    "\n",
    "        if weapon_table:\n",
    "            # Extract data from the table\n",
    "            table_data = []\n",
    "            # Assuming the first row is the header\n",
    "            header_row = weapon_table.find('tr')\n",
    "            if header_row:\n",
    "                headers = [th.get_text(strip=True) for th in header_row.find_all('th')]\n",
    "\n",
    "                # Extract data rows (skip the header row)\n",
    "                data_rows = weapon_table.find_all('tr')[1:]\n",
    "\n",
    "                for row in data_rows:\n",
    "                    cells = row.find_all('td')\n",
    "                    # Ensure we have the expected number of cells\n",
    "                    if len(headers) == len(cells):\n",
    "                        row_data = {}\n",
    "                        # Extract data for each column based on header\n",
    "                        for i, header in enumerate(headers):\n",
    "                            cell = cells[i]\n",
    "                            cell_text = cell.get_text(strip=True)\n",
    "\n",
    "                            # Special handling for the 'Weapon' column to get image and link\n",
    "                            if header == 'Weapon':\n",
    "                                weapon_link_tag = cell.find('a', class_='a-link')\n",
    "                                if weapon_link_tag:\n",
    "                                    row_data['Weapon Name'] = weapon_link_tag.get_text(strip=True)\n",
    "                                    # Construct full URL if href is relative\n",
    "                                    href = weapon_link_tag.get('href')\n",
    "                                    if href:\n",
    "                                         if href.startswith('//'):\n",
    "                                             row_data['Weapon Link'] = 'https:' + href\n",
    "                                         elif href.startswith('/'):\n",
    "                                             row_data['Weapon Link'] = 'https://game8.co' + href\n",
    "                                         else:\n",
    "                                             row_data['Weapon Link'] = href # Assume absolute if no scheme/leading slash\n",
    "\n",
    "\n",
    "                                icon_img_tag = cell.find('img')\n",
    "                                if icon_img_tag:\n",
    "                                    img_url = icon_img_tag.get('data-src') or icon_img_tag.get('src')\n",
    "                                    if img_url:\n",
    "                                        if img_url.startswith('//'):\n",
    "                                            row_data['Weapon Icon URL'] = 'https:' + img_url\n",
    "                                        elif img_url.startswith('/'):\n",
    "                                            row_data['Weapon Icon URL'] = 'https://game8.co' + img_url\n",
    "                                        else:\n",
    "                                            row_data['Weapon Icon URL'] = img_url # Assume absolute\n",
    "                            else:\n",
    "                                # For other columns, just get the text\n",
    "                                row_data[header] = cell_text\n",
    "\n",
    "                        table_data.append(row_data)\n",
    "                    else:\n",
    "                        print(f\"Skipping weapon row due to header/cell mismatch: {headers} vs {[cell.get_text(strip=True) for cell in cells]}\")\n",
    "\n",
    "            # Store the extracted table data under the heading text\n",
    "            weapons_data[heading_text] = table_data\n",
    "        else:\n",
    "            print(f\"No weapon table found after h3 with id='{h3_id}'\")\n",
    "    else:\n",
    "        print(f\"h3 with id='{h3_id}' not found\")\n",
    "\n",
    "# Display the extracted weapon data\n",
    "display(weapons_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5298cd3b-0c7d-46fa-aeca-fa3dd21803af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "csv_data = []\n",
    "for category, weapons in weapons_data.items():\n",
    "    for weapon in weapons:\n",
    "        csv_data.append({\n",
    "            'Weapon Category': category,\n",
    "            'Weapon Name': weapon.get('Weapon Name', ''),\n",
    "            'Weapon Link': weapon.get('Weapon Link', ''),\n",
    "            'Weapon Icon URL': weapon.get('Weapon Icon URL', ''),\n",
    "            'Description': weapon.get('Description', ''),\n",
    "            'Location': weapon.get('Location', '')\n",
    "        })\n",
    "\n",
    "# Convert the list of dictionaries into a pandas DataFrame\n",
    "weapons_df = pd.DataFrame(csv_data)\n",
    "\n",
    "# Define CSV file name\n",
    "csv_file = 'weapons_data.csv'\n",
    "\n",
    "# Write data to CSV\n",
    "weapons_df.to_csv(csv_file, index=False)\n",
    "\n",
    "print(f\"Weapon data saved to '{csv_file}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da8e489-2688-45cc-b59f-99a0941c53be",
   "metadata": {},
   "source": [
    "## Full weapon info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75273a6c-51a7-47a0-a41d-eef251a839ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_weapon_data = []\n",
    "\n",
    "# Iterate through the weapons_df DataFrame\n",
    "for index, row in weapons_df.iterrows():\n",
    "    weapon_name = row['Weapon Name']\n",
    "    weapon_url = row['Weapon Link']\n",
    "\n",
    "    # Skip if the weapon link is empty or not a valid URL\n",
    "    if not weapon_url or not weapon_url.startswith('http'):\n",
    "        print(f\"Skipping {weapon_name} due to missing or invalid URL: {weapon_url}\")\n",
    "        full_weapon_data.append({\n",
    "            'Weapon Name': weapon_name,\n",
    "            'Weapon URL': weapon_url,\n",
    "            'Basic Information': 'N/A', # Use N/A for missing sections\n",
    "            'Best Upgrades': 'N/A',\n",
    "            'How to use': 'N/A',\n",
    "            'How to get': 'N/A' # Add N/A for the new column\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        response = requests.get(weapon_url)\n",
    "        response.raise_for_status() # Raise an exception for bad status codes\n",
    "\n",
    "        weapon_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Dictionary to store detailed information for this weapon\n",
    "        weapon_details = {\n",
    "            'Weapon Name': weapon_name,\n",
    "            'Weapon URL': weapon_url,\n",
    "            'Basic Information': {},\n",
    "            'Best Upgrades': {},\n",
    "            'How to use': \"\", # Assuming How to use is mainly paragraph text\n",
    "            'How to get': \"\" # Initialize the new 'How to get' field\n",
    "        }\n",
    "\n",
    "        # --- Extract Basic Information ---\n",
    "        # This section seems to be under an h2 with id 'hl_1' and contains a table\n",
    "        basic_info_heading = weapon_soup.find('h2', id='hl_1', class_='a-header--2')\n",
    "        if basic_info_heading:\n",
    "            basic_info_table = basic_info_heading.find_next_sibling('table', class_='a-table')\n",
    "            if basic_info_table:\n",
    "                table_data = []\n",
    "                header_row = basic_info_table.find('tr')\n",
    "                if header_row:\n",
    "                    headers = [th.get_text(strip=True) for th in header_row.find_all('th')]\n",
    "                    data_rows = basic_info_table.find_all('tr')[1:]\n",
    "                    for r in data_rows:\n",
    "                        cells = r.find_all('td')\n",
    "                        if len(headers) == len(cells):\n",
    "                            row_data = {}\n",
    "                            for i, header in enumerate(headers):\n",
    "                                # Extract text from td, including nested tags\n",
    "                                cell_text = \"\".join(content.get_text(strip=True) for content in cells[i].contents if isinstance(content, str) or content.name in ['span', 'b', 'a', 'img'])\n",
    "                                row_data[header] = cell_text\n",
    "                            table_data.append(row_data)\n",
    "                weapon_details['Basic Information'] = table_data\n",
    "            else:\n",
    "                # If no table, look for paragraphs under this heading\n",
    "                basic_info_paragraphs = []\n",
    "                next_sibling = basic_info_heading.next_sibling\n",
    "                while next_sibling and (next_sibling.name not in ['h2', 'h3'] if hasattr(next_sibling, 'name') else True): # Check if it's a tag before accessing name\n",
    "                    if hasattr(next_sibling, 'name') and next_sibling.name == 'p' and 'a-paragraph' in next_sibling.get('class', []):\n",
    "                         basic_info_paragraphs.append(next_sibling.get_text(strip=True))\n",
    "                    next_sibling = next_sibling.next_sibling\n",
    "                if basic_info_paragraphs:\n",
    "                    weapon_details['Basic Information'] = {\"paragraphs\": \"\\n\".join(basic_info_paragraphs)}\n",
    "\n",
    "\n",
    "        # --- Extract Best Upgrades ---\n",
    "        # This section seems to be under an h2 with id 'hl_2' and contains an h3 and a table\n",
    "        best_upgrades_heading = weapon_soup.find('h2', id='hl_2', class_='a-header--2')\n",
    "        if best_upgrades_heading:\n",
    "             best_upgrades_sub_heading = best_upgrades_heading.find_next_sibling('h3', id='hm_1', class_='a-header--3') # Assuming hm_1 is the sub-heading\n",
    "             if best_upgrades_sub_heading:\n",
    "                best_upgrades_table = best_upgrades_sub_heading.find_next_sibling('table', class_='a-table')\n",
    "                if best_upgrades_table:\n",
    "                    table_data = []\n",
    "                    header_row = best_upgrades_table.find('tr')\n",
    "                    if header_row:\n",
    "                        headers = [th.get_text(strip=True) for th in header_row.find_all('th')]\n",
    "                        data_rows = best_upgrades_table.find_all('tr')[1:]\n",
    "                        for r in data_rows:\n",
    "                            cells = r.find_all('td')\n",
    "                            if len(headers) == len(cells):\n",
    "                                row_data = {}\n",
    "                                for i, header in enumerate(headers):\n",
    "                                    # Extract text from td, including nested tags\n",
    "                                    cell_text = \"\".join(content.get_text(strip=True) for content in cells[i].contents if isinstance(content, str) or content.name in ['span', 'b', 'a', 'img'])\n",
    "                                    row_data[header] = cell_text\n",
    "                                table_data.append(row_data)\n",
    "                    weapon_details['Best Upgrades'] = table_data\n",
    "                else:\n",
    "                    # If no table, look for paragraphs under this heading\n",
    "                    best_upgrades_paragraphs = []\n",
    "                    next_sibling = best_upgrades_sub_heading.next_sibling\n",
    "                    while next_sibling and (next_sibling.name not in ['h2', 'h3'] if hasattr(next_sibling, 'name') else True): # Check if it's a tag before accessing name\n",
    "                         if hasattr(next_sibling, 'name') and next_sibling.name == 'p' and 'a-paragraph' in next_sibling.get('class', []):\n",
    "                              best_upgrades_paragraphs.append(next_sibling.get_text(strip=True))\n",
    "                         next_sibling = next_sibling.next_sibling\n",
    "                    if best_upgrades_paragraphs:\n",
    "                         weapon_details['Best Upgrades'] = {\"paragraphs\": \"\\n\".join(best_upgrades_paragraphs)}\n",
    "             else:\n",
    "                 # If no h3 sub-heading, look for table or paragraphs directly under h2\n",
    "                 best_upgrades_table = best_upgrades_heading.find_next_sibling('table', class_='a-table')\n",
    "                 if best_upgrades_table:\n",
    "                     table_data = []\n",
    "                     header_row = best_upgrades_table.find('tr')\n",
    "                     if header_row:\n",
    "                         headers = [th.get_text(strip=True) for th in header_row.find_all('th')]\n",
    "                         data_rows = best_upgrades_table.find_all('tr')[1:]\n",
    "                         for r in data_rows:\n",
    "                             cells = r.find_all('td')\n",
    "                             if len(headers) == len(cells):\n",
    "                                 row_data = {}\n",
    "                                 for i, header in enumerate(headers):\n",
    "                                     # Extract text from td, including nested tags\n",
    "                                     cell_text = \"\".join(content.get_text(strip=True) for content in cells[i].contents if isinstance(content, str) or content.name in ['span', 'b', 'a', 'img'])\n",
    "                                     row_data[header] = cell_text\n",
    "                                 table_data.append(row_data)\n",
    "                     weapon_details['Best Upgrades'] = table_data\n",
    "                 else:\n",
    "                     best_upgrades_paragraphs = []\n",
    "                     next_sibling = best_upgrades_heading.next_sibling\n",
    "                     while next_sibling and (next_sibling.name not in ['h2', 'h3'] if hasattr(next_sibling, 'name') else True): # Check if it's a tag before accessing name\n",
    "                          if hasattr(next_sibling, 'name') and next_sibling.name == 'p' and 'a-paragraph' in next_sibling.get('class', []):\n",
    "                               best_upgrades_paragraphs.append(next_sibling.get_text(strip=True))\n",
    "                          next_sibling = next_sibling.next_sibling\n",
    "                     if best_upgrades_paragraphs:\n",
    "                          weapon_details['Best Upgrades'] = {\"paragraphs\": \"\\n\".join(best_upgrades_paragraphs)}\n",
    "\n",
    "\n",
    "        # --- Extract How to use ---\n",
    "        # This section might be under an h2 with id 'hl_3' or similar\n",
    "        how_to_use_heading = weapon_soup.find('h2', text=re.compile(r'How to Use', re.IGNORECASE), class_='a-header--2') # Look for h2 with 'How to Use' in text\n",
    "        if how_to_use_heading:\n",
    "            how_to_use_paragraphs = []\n",
    "            next_sibling = how_to_use_heading.next_sibling\n",
    "            while next_sibling and (next_sibling.name not in ['h2', 'h3'] if hasattr(next_sibling, 'name') else True): # Check if it's a tag before accessing name\n",
    "                if hasattr(next_sibling, 'name') and next_sibling.name == 'p' and 'a-paragraph' in next_sibling.get('class', []):\n",
    "                    how_to_use_paragraphs.append(next_sibling.get_text(strip=True))\n",
    "                next_sibling = next_sibling.next_sibling\n",
    "            weapon_details['How to use'] = \"\\n\".join(how_to_use_paragraphs)\n",
    "\n",
    "\n",
    "        # --- Extract How to get ---\n",
    "        # This section seems to be under an h2 with id 'hl_4'\n",
    "        how_to_get_heading = weapon_soup.find('h2', id='hl_4', class_='a-header--2')\n",
    "        if how_to_get_heading:\n",
    "             how_to_get_content = []\n",
    "             # Collect subsequent h3 and p tags until the next h2 or end of content\n",
    "             next_sibling = how_to_get_heading.next_sibling\n",
    "             while next_sibling and (next_sibling.name not in ['h2'] if hasattr(next_sibling, 'name') else True): # Check if it's a tag before accessing name\n",
    "                  if hasattr(next_sibling, 'name') and next_sibling.name in ['h3', 'p'] and (next_sibling.get('class', []) == ['a-header--3'] or next_sibling.get('class', []) == ['a-paragraph']):\n",
    "                       how_to_get_content.append(next_sibling.get_text(strip=True))\n",
    "                  next_sibling = next_sibling.next_sibling\n",
    "             weapon_details['How to get'] = \"\\n\".join(how_to_get_content)\n",
    "\n",
    "\n",
    "\n",
    "        full_weapon_data.append(weapon_details)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {weapon_url}: {e}\")\n",
    "        full_weapon_data.append({\n",
    "            'Weapon Name': weapon_name,\n",
    "            'Weapon URL': weapon_url,\n",
    "            'Basic Information': f\"Error fetching data: {e}\",\n",
    "            'Best Upgrades': f\"Error fetching data: {e}\",\n",
    "            'How to use': f\"Error fetching data: {e}\",\n",
    "            'How to get': f\"Error fetching data: {e}\"\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while processing {weapon_url}: {e}\")\n",
    "        full_weapon_data.append({\n",
    "            'Weapon Name': weapon_name,\n",
    "            'Weapon URL': weapon_url,\n",
    "            'Basic Information': f\"An unexpected error occurred: {e}\",\n",
    "            'Best Upgrades': f\"An unexpected error occurred: {e}\",\n",
    "            'How to use': f\"An unexpected error occurred: {e}\",\n",
    "            'How to get': f\"An unexpected error occurred: {e}\"\n",
    "        })\n",
    "\n",
    "\n",
    "# Convert the list of dictionaries into a pandas DataFrame\n",
    "full_weapons_df = pd.DataFrame(full_weapon_data)\n",
    "\n",
    "# Display the first few rows to verify the data\n",
    "display(full_weapons_df.head())\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "csv_file = 'full_weapon_info.csv'\n",
    "full_weapons_df.to_csv(csv_file, index=False)\n",
    "\n",
    "print(f\"Full weapon information saved to '{csv_file}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05c28b0-b588-4af7-b65b-52b299864176",
   "metadata": {},
   "source": [
    "## Enemies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45546941-b0b9-490e-af1e-80d5099dd56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "enemies = 'https://game8.co/games/Last-of-Us-2/archives/290294' #enemies page\n",
    "response = requests.get(enemies)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6761c7-6e54-433d-8191-bda46e34ea5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "enemies_soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bc1377-93e3-43ac-9f20-b8f1403c66cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "enemy_data = []\n",
    "\n",
    "# List of h3 IDs to target for enemy sections\n",
    "target_h3_ids = ['hm_1', 'hm_2', 'hm_3', 'hm_4', 'hm_5', 'hm_6', 'hm_7', 'hm_8', 'hm_9', 'hm_10']\n",
    "\n",
    "for h3_id in target_h3_ids:\n",
    "    # Find the specific h3 tag by its ID and class\n",
    "    h3_tag = enemies_soup.find('h3', id=h3_id, class_='a-header--3')\n",
    "\n",
    "    if h3_tag:\n",
    "        enemy_name = h3_tag.get_text(strip=True)\n",
    "\n",
    "        # Find the first paragraph immediately following the h3 tag\n",
    "        first_paragraph = h3_tag.find_next_sibling('p', class_='a-paragraph')\n",
    "\n",
    "        # Find the second paragraph immediately following the first paragraph\n",
    "        second_paragraph = first_paragraph.find_next_sibling('p', class_='a-paragraph') if first_paragraph else None\n",
    "\n",
    "        enemy_image_url = 'No Image Available'\n",
    "        description = 'No Description Available'\n",
    "        how_to_kill_url = 'No URL Available'\n",
    "\n",
    "        if first_paragraph:\n",
    "            # Extract image URL from the first paragraph\n",
    "            img_tag = first_paragraph.find('img')\n",
    "            if img_tag:\n",
    "                img_url = img_tag.get('data-src') or img_tag.get('src')\n",
    "                if img_url:\n",
    "                     if img_url.startswith('//'):\n",
    "                         enemy_image_url = 'https:' + img_url\n",
    "                     elif img_url.startswith('/'):\n",
    "                         enemy_image_url = 'https://game8.co' + img_url\n",
    "                     else:\n",
    "                         enemy_image_url = img_url # Assume absolute\n",
    "\n",
    "\n",
    "            # Extract description text from the first paragraph (excluding image alt text if any)\n",
    "            # Get all text content and join with spaces\n",
    "            description_parts = [content.strip() for content in first_paragraph.contents if isinstance(content, str) or content.name in ['span', 'b']]\n",
    "            description = \" \".join(description_parts).strip()\n",
    "\n",
    "\n",
    "        if second_paragraph:\n",
    "            # Extract the URL from the link in the second paragraph\n",
    "            link_tag = second_paragraph.find('a', class_='a-btn')\n",
    "            if link_tag:\n",
    "                href = link_tag.get('href')\n",
    "                if href:\n",
    "                     if href.startswith('//'):\n",
    "                         how_to_kill_url = 'https:' + href\n",
    "                     elif href.startswith('/'):\n",
    "                         how_to_kill_url = 'https://game8.co' + href\n",
    "                     else:\n",
    "                         how_to_kill_url = href # Assume absolute\n",
    "\n",
    "\n",
    "        enemy_data.append({\n",
    "            'Enemy name': enemy_name,\n",
    "            'enemy image url': enemy_image_url,\n",
    "            'description': description,\n",
    "            'how to kill url': how_to_kill_url\n",
    "        })\n",
    "    else:\n",
    "        print(f\"h3 with id='{h3_id}' not found\")\n",
    "\n",
    "# Display the extracted enemy data (optional)\n",
    "# display(enemy_data)\n",
    "\n",
    "# Convert to DataFrame and save to CSV\n",
    "enemy_df = pd.DataFrame(enemy_data)\n",
    "display(enemy_df.head())\n",
    "\n",
    "csv_file = 'enemy_data.csv'\n",
    "enemy_df.to_csv(csv_file, index=False)\n",
    "\n",
    "print(f\"Enemy data saved to '{csv_file}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b5be28-3000-4a8b-aed5-447ff4133e15",
   "metadata": {},
   "source": [
    "## How to kill enemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54eae237-9add-4074-bdf2-c36f929df979",
   "metadata": {},
   "outputs": [],
   "source": [
    "how_to_kill_details = []\n",
    "\n",
    "# Iterate through the enemy_df DataFrame\n",
    "for index, row in enemy_df.iterrows():\n",
    "    enemy_name = row['Enemy name']\n",
    "    how_to_kill_url = row['how to kill url']\n",
    "\n",
    "    # Skip if the URL is 'No URL Available' or empty\n",
    "    if how_to_kill_url == 'No URL Available' or not how_to_kill_url:\n",
    "        print(f\"Skipping {enemy_name} due to missing How to Kill URL.\")\n",
    "        how_to_kill_details.append({\n",
    "            'Enemy name': enemy_name,\n",
    "            'How to Kill Guide': 'No Guide Available' # Placeholder for missing guide\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        response = requests.get(how_to_kill_url)\n",
    "        response.raise_for_status() # Raise an exception for bad status codes\n",
    "\n",
    "        guide_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Dictionary to store the guide details for this enemy\n",
    "        guide_info_dict = {}\n",
    "        current_heading = None\n",
    "        current_content_list = [] # To store paragraphs and image URLs under the current heading\n",
    "\n",
    "        # Find all h2, h3, p, and img tags within the main content area\n",
    "        # This selects all h2 with class a-header--2, h3 with class a-header--3, p with class a-paragraph, and img tags\n",
    "        content_elements = guide_soup.select('h2.a-header--2, h3.a-header--3, p.a-paragraph, img')\n",
    "\n",
    "        for element in content_elements:\n",
    "            # If it's an h2 or h3 heading\n",
    "            if element.name in ['h2', 'h3']:\n",
    "                # If we have collected content under the previous heading, store it\n",
    "                if current_heading and current_content_list:\n",
    "                     guide_info_dict[current_heading] = current_content_list\n",
    "\n",
    "                current_heading = element.get_text(strip=True)\n",
    "                current_content_list = [] # Reset content list for the new heading\n",
    "\n",
    "            # If it's a paragraph and we have a current heading\n",
    "            elif element.name == 'p' and 'a-paragraph' in element.get('class', []):\n",
    "                if current_heading:\n",
    "                    # Extract all text content from the paragraph, including nested span and a tags\n",
    "                    paragraph_text = \"\".join(content.get_text(strip=True) for content in element.contents if isinstance(content, str) or content.name in ['span', 'b', 'a'])\n",
    "                    if paragraph_text:\n",
    "                         current_content_list.append({'type': 'paragraph', 'content': paragraph_text})\n",
    "\n",
    "            # If it's an image and we have a current heading\n",
    "            elif element.name == 'img':\n",
    "                 if current_heading:\n",
    "                     img_url = element.get('data-src') or element.get('src')\n",
    "                     if img_url:\n",
    "                         if img_url.startswith('//'):\n",
    "                             img_url = 'https:' + img_url\n",
    "                         elif img_url.startswith('/'):\n",
    "                             img_url = 'https://game8.co' + img_url\n",
    "                         current_content_list.append({'type': 'image', 'url': img_url})\n",
    "\n",
    "\n",
    "        # Add the last collected content\n",
    "        if current_heading and current_content_list:\n",
    "             guide_info_dict[current_heading] = current_content_list\n",
    "\n",
    "        # Remove unwanted sections (like Comments and Author) after collection\n",
    "        unwanted_headings = ['Comment', 'Author']\n",
    "        for heading in unwanted_headings:\n",
    "            guide_info_dict.pop(heading, None)\n",
    "\n",
    "        # Explicitly remove the 'Last of Us 2 Enemies' entry if it exists\n",
    "        guide_info_dict.pop('Last of Us 2 Enemies', None)\n",
    "\n",
    "\n",
    "        how_to_kill_details.append({\n",
    "            'Enemy name': enemy_name,\n",
    "            'How to Kill Guide': guide_info_dict # Store the dictionary\n",
    "        })\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {how_to_kill_url}: {e}\")\n",
    "        how_to_kill_details.append({\n",
    "            'Enemy name': enemy_name,\n",
    "            'How to Kill Guide': f\"Error fetching data: {e}\" # Indicate error\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while processing {how_to_kill_url}: {e}\")\n",
    "        how_to_kill_details.append({\n",
    "            'Enemy name': enemy_name,\n",
    "            'How to Kill Guide': f\"An unexpected error occurred: {e}\" # Indicate error\n",
    "        })\n",
    "\n",
    "# Now how_to_kill_details is a list of dictionaries with enemy names and their guides\n",
    "# You can merge this with the enemy_df DataFrame\n",
    "how_to_kill_df = pd.DataFrame(how_to_kill_details)\n",
    "\n",
    "# Merge with the original enemy_df\n",
    "merged_enemy_df = pd.merge(enemy_df, how_to_kill_df, on='Enemy name', how='left')\n",
    "\n",
    "# Display the merged DataFrame\n",
    "display(merged_enemy_df.head())\n",
    "\n",
    "# You can save the merged DataFrame to a CSV\n",
    "# merged_enemy_df.to_csv('enemy_data_with_guides.csv', index=False)\n",
    "# print(\"Enemy data with guides saved to 'enemy_data_with_guides.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a474dd71-3e01-4828-8fdb-d0138c914e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CSV file name for the merged enemy data\n",
    "csv_file = 'how_to_kill_enemy.csv'\n",
    "\n",
    "# Save the merged DataFrame to a CSV file\n",
    "merged_enemy_df.to_csv(csv_file, index=False)\n",
    "\n",
    "print(f\"Enemy data with guides saved to '{csv_file}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
