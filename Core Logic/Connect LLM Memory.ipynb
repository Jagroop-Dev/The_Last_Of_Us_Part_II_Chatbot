{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5b31446-6b45-4ae8-bf07-dd72c8d4b1b8",
   "metadata": {},
   "source": [
    "# Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861f8165-fbed-4e26-a79b-2554b31d6a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\js208\\anaconda3\\lib\\site-packages (0.2.1)\n"
     ]
    }
   ],
   "source": [
    "#!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c659a5e-5b5d-4209-95f9-bb652cc17159",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ca7c0d3-f465-4737-9ca2-a4d0e0fcefc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace \n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c85cc1d-4814-41f3-8a84-8db05d24e314",
   "metadata": {},
   "source": [
    "# Setting up LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7c0ca1-b96e-46d1-ab35-85b732eca26a",
   "metadata": {},
   "source": [
    "## Importing the .Env variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ffe69a4-44b4-40ac-b229-cb1ddcf46aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN=os.environ.get('HF_TOKEN')\n",
    "huggingface_repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836ca4c5-11fe-4d68-aa93-2a0ae51a96ee",
   "metadata": {},
   "source": [
    "keeping the `temperature` low makes is accurate and very concise in the answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1034f25c-7668-4901-96fa-20eb2d9c4a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llm(huggingface_repo_id):\n",
    "    llm = HuggingFaceEndpoint(  \n",
    "        repo_id=huggingface_repo_id, \n",
    "        temperature=0.5,\n",
    "        huggingfacehub_api_token=HF_TOKEN,\n",
    "        task='conversational'\n",
    "    )\n",
    "    return llm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fda8eb9-72e8-428b-b508-70f6564881a0",
   "metadata": {},
   "source": [
    "# Connecting to LLM and creating a chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c442cd9c-1434-47fd-a207-5a8ccf17e86d",
   "metadata": {},
   "source": [
    "## Setting the prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1434bed9-0c7a-47cd-a936-5abc18da647a",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_prompt_template = \"\"\"\n",
    "Use the pieces of information provided in the context to answer user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "Don't provide anything out of the given context\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Start the answer directly. No small talk please.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2610b6fa-6412-4bdf-b6ed-94a1028bdb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_custom_prompt(custom_prompt_template):\n",
    "    prompt= PromptTemplate(template=custom_prompt_template,\n",
    "                          input_variables=[\"context\", \"question\"])\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb050c9-0d81-4550-b39e-4bde0984356b",
   "metadata": {},
   "source": [
    "## Loading the FAISS Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4812cc73-e5d4-4100-a534-68cec5f77c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model= HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L12-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04c64dcf-0bcd-4c1e-840c-38ab2bfa657b",
   "metadata": {},
   "outputs": [],
   "source": [
    "FAISS_DB_PATH = 'FAISS Database/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b846488-e3e6-4482-80a7-0a0677002e77",
   "metadata": {},
   "source": [
    "Set the `allow_dangerous_deserialization` parameter to `True` becuase we trust the `FAISS` database\n",
    "and its contents inside it because we made it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f918e91-e520-4f70-89da-cd6803758b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.load_local(FAISS_DB_PATH,\n",
    "                      embedding_model,\n",
    "                      allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93782288-67d1-4a58-8f25-6ada73131153",
   "metadata": {},
   "source": [
    "## Creating the QA Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71aa226b-368b-4046-9a87-ab480b617bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = ChatHuggingFace(llm=load_llm(huggingface_repo_id))\n",
    "qa_chain=RetrievalQA.from_chain_type(\n",
    "    llm=chat_model,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever(search_kwargs={'k':3}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={'prompt':set_custom_prompt(custom_prompt_template)}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5932dee6-9cb7-40a1-828e-ad0e00b64a63",
   "metadata": {},
   "source": [
    "## Creating user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "008e3274-b23c-47a4-86b4-3119a044685a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ask WLF  when is gta 6 gettng released\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result :   I'm sorry for any confusion, but the context provided doesn't contain any information about the release date of GTA 6. The information in the context pertains to a video game walkthrough, not real-world events or game release dates.\n",
      "Source Documents:  [Document(id='8e81b396-5d92-43a1-8cca-b818265e35ab', metadata={}, page_content='already picked up the page, but it will be on the filing cabinet in front of the coffee machine. The note will talk about the code being the “Big Win”. From here, just turn around to the bulletin board and there will be a “TERRA LOTTO” ticket. The code will be the circled numbers. Inside, you will find Ammo and a new gun, the Hunting Pistol .'), Document(id='e2477a0d-f840-48c1-8f86-23b0c8376e88', metadata={}, page_content=\"Vault Code Once inside the vault you will find a dead body with a shotgun on it, and a note about the guy getting trapped inside the vault. Put away the note after reading it, and enjoy your new weapon! (Note: it is still possible to get this weapon, even if you don't unlock the vault here).\"), Document(id='d57769bb-970d-4528-827a-28730a47d9bd', metadata={}, page_content='Super Market Safe - \\nA safe is in the office with a letter that gives a hint to the combo.\\n\\nThrowing the Cable - \\nAfter taking out the enemies in the area, you will find a cable underneath a large fan. Pick it up and throw it over the air duct so that you can climb up. Throwing is the same as throwing molotovs and other items, which is by default done by holding L2, adjusting the arc, then pressing R2. If you miss, you can pick up the cable and try again.')]\n"
     ]
    }
   ],
   "source": [
    "user_query=input(\"Ask WLF \")\n",
    "response=qa_chain.invoke({'query': user_query})\n",
    "print(\"Result : \", response[\"result\"])\n",
    "print(\"Source Documents: \", response[\"source_documents\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
